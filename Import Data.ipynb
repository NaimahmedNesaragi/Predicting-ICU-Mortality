{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data and Clean/Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as npb\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import statsmodels.imputation.mice as smi\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in files from train, test validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import errno\n",
    "path = 'ICU_Mini/*.txt'\n",
    "mini_files = glob.glob(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import errno\n",
    "path = 'ICU_Data/*.txt'\n",
    "files = glob.glob(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Test_Data/*.txt'\n",
    "test_files = glob.glob(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Validation_Data/*.txt'\n",
    "val_files = glob.glob(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in a mountain of text data!\n",
    "def read_mountain_data(files):\n",
    "    counts = pd.DataFrame()\n",
    "    vital_means = pd.DataFrame()\n",
    "    vital_std = pd.DataFrame()\n",
    "    with tqdm(total=100) as pbar:\n",
    "        for file in sorted(files):\n",
    "            with open(file) as f:\n",
    "                content = f.readlines()\n",
    "            contents = [i.rstrip().split(',') for i in content]\n",
    "            data = pd.DataFrame(contents[1:])\n",
    "            data[2] = data[2].astype(float)\n",
    "            counts = counts.append(data.drop(columns = 2).groupby(1).count().transpose(), sort=True)\n",
    "            vital_means = vital_means.append(data.groupby(1).mean().transpose(), sort=True)\n",
    "            pbar.update(.025)\n",
    "        return [vital_means, counts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in standard deviations, and clean data in process\n",
    "def read_standard_deviations(files):\n",
    "    vital_std = pd.DataFrame()\n",
    "    with tqdm(total=100) as pbar:\n",
    "        for file in sorted(files):\n",
    "            with open(file) as f:\n",
    "                content = f.readlines()\n",
    "            contents = [i.rstrip().split(',') for i in content]\n",
    "            del contents[0]\n",
    "            filtered = [x for x in contents if not (((x[1] == 'NIDiasABP') & (float(x[2]) < 10.))\n",
    "                                        |((x[1] == 'NISysABP') & (float(x[2]) < 10.))\n",
    "                                        |((x[1] == 'NIMAP') & (float(x[2]) < 10.)) \n",
    "                                       |((x[1] == 'Height') & (float(x[2]) < 90.))\n",
    "                                       |((x[1] == 'Weight') & (float(x[2]) < 30.))\n",
    "                                       |((x[1] == 'Gender') & (float(x[2]) < 0.)))]\n",
    "            data = pd.DataFrame(filtered)\n",
    "            data[2] = data[2].astype(float)\n",
    "            vital_std = vital_std.append(data.groupby(1).std().transpose(), sort=True)\n",
    "            pbar.update(.025)\n",
    "        return vital_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.00000000000561it [01:50,  1.08it/s]                          \n"
     ]
    }
   ],
   "source": [
    "data = read_mountain_data(files)\n",
    "train_data = data[0]\n",
    "counts = data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.00000000000561it [02:09,  2.81s/it]                          \n"
     ]
    }
   ],
   "source": [
    "data = read_mountain_data(test_files)\n",
    "test_means = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.00000000000561it [01:53,  1.13s/it]                          \n"
     ]
    }
   ],
   "source": [
    "data = read_mountain_data(val_files)\n",
    "val_means = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.00000000000561it [00:41,  2.39it/s]                          \n"
     ]
    }
   ],
   "source": [
    "train_std = read_standard_deviations(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.00000000000561it [00:40,  2.47it/s]                          \n",
      "100.00000000000561it [00:52,  1.89it/s]                          \n"
     ]
    }
   ],
   "source": [
    "test_std = read_standard_deviations(test_files)\n",
    "val_std = read_standard_deviations(val_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_std = val_std.drop(columns = '');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"all_feature_means.pickle\",\"rb\")\n",
    "train_data = pickle.load(pickle_in)\n",
    "pickle_in = open(\"test_data.pickle\",\"rb\")\n",
    "test_data = pickle.load(pickle_in)\n",
    "pickle_in = open(\"val_data.pickle\",\"rb\")\n",
    "val_data = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More cleaning of impossible values: 0 Blood pressure (all patients were alive during the 48 hours, so it must simply mean a machine was disconnected).\n",
    "Also, all patients were adults (at least 15 years old), so removing impossible weights/heights (under 80 cm and under 30 kg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.loc[(train_data.Height < 90),'Height'] = np.NaN\n",
    "train_data.loc[(train_data.Gender < 0),'Gender'] = np.NaN\n",
    "train_data.loc[(train_data.Weight < 30),'Weight'] = np.NaN\n",
    "test_data.loc[(test_data.Height < 90),'Height'] = np.NaN\n",
    "test_data.loc[(test_data.Gender < 0),'Gender'] = np.NaN\n",
    "test_data.loc[(test_data.Weight < 30),'Weight'] = np.NaN\n",
    "val_data.loc[(val_data.Height < 90),'Height'] = np.NaN\n",
    "val_data.loc[(val_data.Gender < 0),'Gender'] = np.NaN\n",
    "val_data.loc[(val_data.Weight < 30),'Weight'] = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping MechVent, the training data was identical for all patients\n",
    "train_data = train_data.drop(columns = 'MechVent')\n",
    "test_data = test_data.drop(columns = 'MechVent')\n",
    "val_data = val_data.drop(columns = 'MechVent')\n",
    "val_data = val_data.drop(columns = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging Invasive/Noninvasive BP's by keeping invasive if available, replacing with NI only when necessary\n",
    "bp = ['DiasABP','SysABP','MAP']\n",
    "NIbp = ['NIDiasABP','NISysABP','NIMAP']\n",
    "\n",
    "def merge_BP(df):\n",
    "    for index, col in enumerate(bp):\n",
    "        df[col].fillna(df[NIbp[index]], inplace=True)\n",
    "        del df[NIbp[index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_BP(train_data)\n",
    "merge_BP(test_data)\n",
    "merge_BP(val_data)\n",
    "merge_BP(train_std)\n",
    "merge_BP(test_std)\n",
    "merge_BP(val_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing impossibly low BP values while at it\n",
    "train_data.loc[(train_data.DiasABP < 10),'DiasABP'] = np.NaN\n",
    "train_data.loc[(train_data.SysABP < 10),'SysABP'] = np.NaN\n",
    "train_data.loc[(train_data.MAP < 10),'MAP'] = np.NaN\n",
    "test_data.loc[(test_data.DiasABP < 10),'DiasABP'] = np.NaN\n",
    "test_data.loc[(test_data.SysABP < 10),'SysABP'] = np.NaN\n",
    "test_data.loc[(test_data.MAP < 10),'MAP'] = np.NaN\n",
    "val_data.loc[(val_data.DiasABP < 10),'DiasABP'] = np.NaN\n",
    "val_data.loc[(val_data.SysABP < 10),'SysABP'] = np.NaN\n",
    "val_data.loc[(val_data.MAP < 10),'MAP'] = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in outcomes and pretty them up for use\n",
    "def find_outcomes(text):\n",
    "    with open(text) as f:\n",
    "        content = f.readlines()\n",
    "        contents = [i.rstrip().split(',') for i in content[1:]]\n",
    "    outcomes = [[i[0],i[5]] for i in contents]\n",
    "    outcomes = pd.DataFrame(outcomes)\n",
    "    outcomes = outcomes.rename(columns = {0:'RecordID',1:'Outcome'})\n",
    "    outcomes = outcomes.astype(int)\n",
    "    return outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_outcomes = find_outcomes('Outcomes.txt')\n",
    "test_outcomes = find_outcomes('outcomes_test.txt')\n",
    "val_outcomes = find_outcomes('outcomes_val.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reducing Features down\n",
    "categories = ['Age','BUN', 'Creatinine', 'DiasABP', 'FiO2', 'GCS', 'Glucose', 'HCO3',\n",
    "       'HCT', 'HR', 'ICUType', 'K', 'MAP', 'Mg',\n",
    "       'Na', 'PaCO2', 'PaO2', 'Platelets','Gender',\n",
    "       'RecordID', 'SysABP', 'Temp', 'Urine', 'WBC', 'Weight', 'pH']\n",
    "stdgories = ['BUN', 'Creatinine', 'DiasABP', 'FiO2', 'GCS','Glucose', 'HCO3', 'HCT', 'HR','K',\n",
    "       'MAP','Mg', 'Na',\n",
    "       'PaCO2', 'PaO2', 'Platelets', 'RecordID','SysABP',\n",
    "       'Temp', 'Urine', 'WBC', 'pH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_impute = train_data[categories]\n",
    "for_impute.RecordID = for_impute.RecordID.astype(int)\n",
    "for_imptest = test_data[categories]\n",
    "for_imptest.RecordID = for_imptest.RecordID.astype(int)\n",
    "for_impval = val_data[categories]\n",
    "for_impval.RecordID = for_impval.RecordID.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"for_impute\",\"wb\")\n",
    "pickle.dump(for_impute, pickle_out)\n",
    "pickle_out.close()\n",
    "pickle_out = open(\"for_imptest\",\"wb\")\n",
    "pickle.dump(for_imptest, pickle_out)\n",
    "pickle_out.close()\n",
    "pickle_out = open(\"for_impval\",\"wb\")\n",
    "pickle.dump(for_impval, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isaackim/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for_impstd = train_std[stdgories]\n",
    "for_impstd['RecordID'] = for_impute['RecordID']\n",
    "for_impstd.columns = for_impstd.columns.map(lambda x: str(x) + '_std')\n",
    "for_impstd = for_impstd.rename(columns = {'RecordID_std':'RecordID'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isaackim/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for_teststd = test_std[stdgories]\n",
    "for_teststd['RecordID'] = for_imptest['RecordID']\n",
    "for_teststd.columns = for_teststd.columns.map(lambda x: str(x) + '_std')\n",
    "for_teststd = for_teststd.rename(columns = {'RecordID_std':'RecordID'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isaackim/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for_valstd = val_std[stdgories]\n",
    "for_valstd['RecordID'] = for_impval['RecordID']\n",
    "for_valstd.columns = for_valstd.columns.map(lambda x: str(x) + '_std')\n",
    "for_valstd = for_valstd.rename(columns = {'RecordID_std':'RecordID'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute data with means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "regtrain = pd.merge(for_impute.fillna(for_impute.mean()),train_outcomes, on='RecordID')\n",
    "regtest = pd.merge(for_imptest.fillna(for_imptest.mean()),test_outcomes, on='RecordID')\n",
    "regval = pd.merge(for_impval.fillna(for_impval.mean()),val_outcomes, on='RecordID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"regtrain.pickle\",\"wb\")\n",
    "pickle.dump(regtrain, pickle_out)\n",
    "pickle_out.close()\n",
    "pickle_out = open(\"regtest.pickle\",\"wb\")\n",
    "pickle.dump(regtest, pickle_out)\n",
    "pickle_out.close()\n",
    "pickle_out = open(\"regval.pickle\",\"wb\")\n",
    "pickle.dump(regval, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use MICE to impute missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"for_impute\",\"rb\")\n",
    "for_impute = pickle.load(pickle_in)\n",
    "pickle_in = open(\"for_imptest\",\"rb\")\n",
    "for_imptest = pickle.load(pickle_in)\n",
    "pickle_in = open(\"for_impval\",\"rb\")\n",
    "for_impval = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use MICE to impute date\n",
    "def impute(data, outcomes):\n",
    "    imp = smi.MICEData(pd.merge(data, outcomes, on='RecordID'))\n",
    "    imp.update_all(20)\n",
    "    impute = imp.data\n",
    "    return impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed = impute(for_impute, train_outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_test = impute(for_imptest, test_outcomes)\n",
    "imputed_val = impute(for_impval, val_outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"imputed.pickle\",\"wb\")\n",
    "pickle.dump(imputed, pickle_out)\n",
    "pickle_out.close()\n",
    "pickle_out = open(\"imputed_test.pickle\",\"wb\")\n",
    "pickle.dump(imputed_test, pickle_out)\n",
    "pickle_out.close()\n",
    "pickle_out = open(\"imputed_val.pickle\",\"wb\")\n",
    "pickle.dump(imputed_val, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_std = impute(for_impstd, train_outcomes)\n",
    "imp_std_test = impute(for_teststd, test_outcomes)\n",
    "imp_std_val = impute(for_valstd, val_outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"imputed_std.pickle\",\"wb\")\n",
    "pickle.dump(imputed_std, pickle_out)\n",
    "pickle_out.close()\n",
    "pickle_out = open(\"imp_std_test.pickle\",\"wb\")\n",
    "pickle.dump(imp_std_test, pickle_out)\n",
    "pickle_out.close()\n",
    "pickle_out = open(\"imp_std_val.pickle\",\"wb\")\n",
    "pickle.dump(imp_std_val, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"imputed_std.pickle\",\"rb\")\n",
    "imputed_std = pickle.load(pickle_in)\n",
    "pickle_in = open(\"imp_std_test.pickle\",\"rb\")\n",
    "imp_std_test = pickle.load(pickle_in)\n",
    "pickle_in = open(\"imp_std_val.pickle\",\"rb\")\n",
    "imp_std_val = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdgories = ['HR_std','WBC_std','RecordID','Outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train = pd.merge(imputed.drop(columns = 'Outcome'), imputed_std[stdgories], on='RecordID')\n",
    "all_test = pd.merge(imputed_test.drop(columns = 'Outcome'), imp_std_test[stdgories], on='RecordID')\n",
    "all_val = pd.merge(imputed_val.drop(columns = 'Outcome'), imp_std_val[stdgories], on='RecordID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"all_train.pickle\",\"wb\")\n",
    "pickle.dump(all_train, pickle_out)\n",
    "pickle_out.close()\n",
    "pickle_out = open(\"all_test.pickle\",\"wb\")\n",
    "pickle.dump(all_test, pickle_out)\n",
    "pickle_out.close()\n",
    "pickle_out = open(\"all_val.pickle\",\"wb\")\n",
    "pickle.dump(all_val, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"all_train.pickle\",\"rb\")\n",
    "all_train = pickle.load(pickle_in)\n",
    "pickle_in = open(\"all_test.pickle\",\"rb\")\n",
    "all_test = pickle.load(pickle_in)\n",
    "pickle_in = open(\"all_val.pickle\",\"rb\")\n",
    "all_val = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age', 'BUN', 'Creatinine', 'DiasABP', 'FiO2', 'GCS', 'Glucose', 'HCO3',\n",
       "       'HCT', 'HR', 'ICUType', 'K', 'MAP', 'Mg', 'Na', 'PaCO2', 'PaO2',\n",
       "       'Platelets', 'Gender', 'RecordID', 'SysABP', 'Temp', 'Urine', 'WBC',\n",
       "       'Weight', 'pH', 'HR_std', 'WBC_std', 'Outcome'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"reduced_train\",\"wb\")\n",
    "pickle.dump(reduced_train, pickle_out)\n",
    "pickle_out.close()\n",
    "pickle_out = open(\"reduced_test\",\"wb\")\n",
    "pickle.dump(reduced_test, pickle_out)\n",
    "pickle_out.close()\n",
    "pickle_out = open(\"reduced_val\",\"wb\")\n",
    "pickle.dump(reduced_val, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Anomaly Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"all_importance_train\",\"rb\")\n",
    "anomaly_tra = pickle.load(pickle_in)\n",
    "pickle_in = open(\"all_importance_test\",\"rb\")\n",
    "anomaly_tes = pickle.load(pickle_in)\n",
    "pickle_in = open(\"all_importance_val\",\"rb\")\n",
    "anomaly_val = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging Invasive/Noninvasive BP's by keeping invasive if available, replacing with NI only when necessary\n",
    "bp = ['DiasABP_I','SysABP_I','MAP_I','DiasABP_A','SysABP_A','MAP_A']\n",
    "NIbp = ['NIDiasABP_I','NISysABP_I','NIMAP_I','NIDiasABP_A','NISysABP_A','NIMAP_A']\n",
    "\n",
    "def merge_BP(df):\n",
    "    for index, col in enumerate(bp):\n",
    "        df[col].fillna(df[NIbp[index]], inplace=True)\n",
    "        del df[NIbp[index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_BP(anomaly_tr)\n",
    "merge_BP(anomaly_tes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_tr = anomaly_tra.reset_index().rename(columns = {'index':'RecordID'}).replace(float(-1),np.NaN)\n",
    "anomaly_te = anomaly_tes.reset_index().rename(columns = {'index':'RecordID'}).replace(float(-1),np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_tr = anomaly_tr.replace(np.inf,np.NaN)\n",
    "anomaly_te = anomaly_te.replace(np.inf,np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_train = impute(anomaly_tr, train_outcomes)\n",
    "anomaly_test = impute(anomaly_te, test_outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"anomaly_train.pickle\",\"wb\")\n",
    "pickle.dump(anomaly_train, pickle_out)\n",
    "pickle_out.close()\n",
    "pickle_out = open(\"anomaly_test.pickle\",\"wb\")\n",
    "pickle.dump(anomaly_test, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomgories = ['DiasABP_A', 'MAP_A','RecordID','Outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train = pd.merge(all_train.drop(columns = 'Outcome'), anomaly_train[anomgories], on='RecordID')\n",
    "final_test = pd.merge(all_test.drop(columns = 'Outcome'), anomaly_test[anomgories], on='RecordID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"final_train.pickle\",\"wb\")\n",
    "pickle.dump(final_train, pickle_out)\n",
    "pickle_out.close()\n",
    "pickle_out = open(\"final_test.pickle\",\"wb\")\n",
    "pickle.dump(final_test, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_all = pd.concat([all_train, all_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"mega_all\",\"wb\")\n",
    "pickle.dump(all_all, pickle_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metis",
   "language": "python",
   "name": "metis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
